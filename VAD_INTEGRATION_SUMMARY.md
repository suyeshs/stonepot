# VAD Integration Summary

## Workflow Validation Complete âœ…

I've analyzed the current audio processing pipeline and validated the workflow for integrating Voice Activity Detection (VAD) with Silero.

---

## Key Findings

### Current Problem
**All audio is continuously streamed to Gemini**, regardless of whether speech is present:

- **File**: [public/audio-processor.js:53-64](stonepot-restaurant-client/public/audio-processor.js#L53-L64)
- **Issue**: Every audio chunk is sent via `this.port.postMessage({ type: 'audio-data', data: pcmData })`
- **Current RMS detection**: Only used for UI state (listening/thinking/speaking), NOT for filtering audio
- **Cost**: â‚¹8-12/hour (billing for silence + background noise)

### Audio Flow
```
Microphone (16kHz)
  â†’ AudioWorklet (converts Float32 â†’ PCM16)
  â†’ Main thread
  â†’ VertexAILiveService.sendAudio()
  â†’ Backend â†’ Gemini API

ðŸ’¸ ALL chunks are sent, even silence and noise
```

---

## Recommended Solution

### Integration Approach: **VAD in Main Thread** (Phase 1)

**Why**:
- Faster implementation (1-2 days vs 3-4 days)
- Easier ONNX Runtime integration
- No AudioWorklet global scope restrictions
- Acceptable latency (<100ms)

**How it works**:
1. AudioWorklet sends raw Float32 audio chunks to main thread
2. Main thread runs Silero VAD on each chunk
3. **Only if speech detected** (probability > 0.5): Convert to PCM16 and send to Gemini
4. Add 800ms silence buffer to avoid cutting off mid-sentence

### Expected Results
- **Cost**: â‚¹2-3/hour (60-80% reduction)
- **Latency**: <100ms from speech start to Gemini
- **Accuracy**: 99.2% (Silero VAD benchmark)
- **False positives**: <1%

---

## Integration Points

### 1. Model Loading
**File**: New file `app/services/VADService.ts`
- Load Silero VAD ONNX model (1.8MB)
- Source: https://github.com/snakers4/silero-vad
- Use `onnxruntime-web` library

### 2. AudioWorklet Changes
**File**: [public/audio-processor.js](stonepot-restaurant-client/public/audio-processor.js)
- **Remove**: `this.port.postMessage({ type: 'audio-data', ... })` (line 61-64)
- **Add**: Send raw Float32 chunks to main thread for VAD processing
- Keep RMS detection for interruption handling

### 3. Main Thread Changes
**File**: [app/page.tsx:519-561](stonepot-restaurant-client/app/page.tsx#L519-L561)
- Run VAD on received Float32 chunks
- Only convert to PCM16 and send to Gemini if speech detected
- Send `turn_complete` after 800ms of silence

---

## Technical Challenges Identified

### 1. Silero VAD Input Requirements
- **Input shape**: `[1, 512]` (512 samples per chunk)
- **Sample rate**: 16kHz âœ… (already configured)
- **Format**: Float32Array âœ… (AudioWorklet native format)
- **Solution**: Accumulate 512 samples before running VAD

### 2. Silence Buffering
- **Problem**: VAD may detect silence during mid-sentence pauses
- **Solution**: Continue sending audio for up to 800ms of detected silence
- **Implementation**: Counter with `maxSilenceFrames = 16` (800ms Ã· 50ms per chunk)

### 3. Turn Completion
- **Problem**: When to signal Gemini that user finished speaking?
- **Solution**: Send `vertexAILiveServiceRef.current.sendTurnComplete()` after sustained silence
- **Threshold**: 800ms (recommended by Gemini docs)

---

## Next Steps

### Phase 1: VAD Integration (1-2 days)
1. Add Silero VAD ONNX model to `/public/models/silero_vad.onnx`
2. Install `onnxruntime-web` dependency
3. Create `app/services/VADService.ts`
4. Modify `audio-processor.js` to send raw Float32 chunks
5. Update `page.tsx` to run VAD before sending to Gemini
6. Test and measure cost savings

### Phase 2: AudioWorklet Optimization (Optional, 2-3 days)
- Move VAD to AudioWorklet for <50ms latency
- Use SharedArrayBuffer for model weights
- Fallback to main thread if browser doesn't support

### Phase 3: Wake Word Detection (1 week)
- Integrate Porcupine or custom TFLite model
- Add "Hey Stonepot" or custom trigger phrase
- Update UI to show wake word status

---

## Files to Modify

| File | Changes | Priority |
|------|---------|----------|
| `package.json` | Add `onnxruntime-web` | P0 |
| `public/models/silero_vad.onnx` | Download model | P0 |
| `app/services/VADService.ts` | Create VAD service | P0 |
| `app/config/vad.ts` | VAD configuration | P0 |
| `public/audio-processor.js` | Send Float32 chunks | P0 |
| `app/page.tsx` | Run VAD in main thread | P0 |
| `app/services/VertexAILiveService.ts` | Add turn complete logging | P1 |

---

## Testing Checklist

### Functional Tests
- [ ] Silent room: No audio sent to Gemini
- [ ] Background noise: Ignored (cafÃ© ambiance, music)
- [ ] Speech detection: Within 50ms of speaking
- [ ] Mid-sentence pauses: Not treated as turn completion
- [ ] Interruption: Works when user speaks during AI response

### Performance Tests
- [ ] Cost reduction: 60-80% fewer bytes sent to Gemini
- [ ] Latency: <100ms from speech start to Gemini
- [ ] Accuracy: >95% speech detection rate
- [ ] False positive rate: <1%

### Browser Compatibility
- [ ] Chrome/Edge (desktop + mobile)
- [ ] Safari (desktop + mobile)
- [ ] Firefox

---

## Cost Savings Projection

### Current State (Without VAD)
- **Session duration**: 1 hour
- **User speaks**: 10 minutes (17%)
- **AI responds**: 10 minutes (17%)
- **Silence/noise**: 40 minutes (67%)
- **Audio sent to Gemini**: 60 minutes (100%)
- **Cost**: â‚¹8-12/hour

### With VAD
- **Audio sent to Gemini**: 15-20 minutes (25-33%) - includes silence buffering
- **Cost**: â‚¹2-3/hour
- **Savings**: â‚¹5-9/hour per session

**Annual savings** (1000 hours/year): â‚¹5,000 - â‚¹9,000

---

## Documentation

Full technical analysis available in: [VAD_WORKFLOW_VALIDATION.md](VAD_WORKFLOW_VALIDATION.md)

Includes:
- Complete audio pipeline diagram
- Current vs proposed architecture
- Code-level integration examples
- Migration plan
- Technical challenges and solutions

---

**Status**: âœ… Workflow Validated
**Ready to Implement**: Yes
**Recommendation**: Start with Phase 1 (VAD in main thread)
**Estimated Time**: 1-2 days development + 1 day testing
